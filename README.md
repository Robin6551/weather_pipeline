# Weather Data Pipeline ğŸŒ¦ï¸

An end-to-end **ETL data pipeline** that extracts weather data from an external API, transforms it into a structured format, and loads it into **PostgreSQL**, orchestrated using **Apache Airflow** and containerized with **Docker**.

This project showcases core data engineering skills such as API ingestion, transformation logic, orchestration, and production-style project structure.

---

## ğŸš€ Project Overview

The pipeline performs the following steps:

1. **Extract** weather data from a public weather API
2. **Transform** raw API responses into clean, analytics-ready data
3. **Load** processed data into PostgreSQL tables
4. Orchestrate the workflow using **Apache Airflow DAGs**

---

## ğŸ§± Architecture

Weather API
â†“
Extract (Python)
â†“
Transform (Python)
â†“
Load (PostgreSQL)
â†“
Airflow DAG (Scheduling & Monitoring)

yaml
Copy code

---

## ğŸ›  Tech Stack

- **Python**
- **Apache Airflow**
- **PostgreSQL**
- **SQL**
- **Docker & Docker Compose**
- **Git & GitHub**

---

## ğŸ“ Project Structure

WEATHER_PIPELINE/
â”‚
â”œâ”€â”€ dags/
â”‚ â”œâ”€â”€ weather_pipeline_dags.py # Airflow DAG definition
â”‚ â””â”€â”€ src/ # DAG task logic
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ config.py # API & DB configuration
â”‚ â”œâ”€â”€ extract.py # Extract weather data
â”‚ â”œâ”€â”€ transform.py # Data cleaning & transformation
â”‚ â”œâ”€â”€ load.py # Load data into PostgreSQL
â”‚ â””â”€â”€ init.py
â”‚
â”œâ”€â”€ sql/ # SQL scripts (optional)
â”œâ”€â”€ logs/ # Airflow logs
â”œâ”€â”€ scripts/ # Helper scripts
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env # Environment variables (ignored)
â””â”€â”€ README.md

yaml
Copy code

---

## ğŸ“¥ Extract

- Fetches current weather data from an external API
- Handles JSON responses and missing fields
- Supports configurable locations via `config.py`

---

## ğŸ”„ Transform

- Normalizes temperature, humidity, and weather metrics
- Converts timestamps to standard formats
- Prepares data for relational storage

---

## ğŸ“¤ Load

- Inserts transformed data into PostgreSQL tables
- Ensures schema consistency and data integrity

---

## â± Orchestration with Airflow

- DAG defined in `weather_pipeline_dags.py`
- Tasks:
  - Extract
  - Transform
  - Load
- Supports scheduled execution and retries

---

## â–¶ï¸ How to Run

### 1. Clone repository
```bash
git clone https://github.com/Robin6551/weather_pipeline.git
cd weather_pipeline
2. Configure environment variables
Create a .env file:

env
Copy code
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=weather
WEATHER_API_KEY=your_api_key
3. Start services
bash
Copy code
docker-compose up -d
4. Access Airflow UI
arduino
Copy code
http://localhost:8080
Trigger the Weather Pipeline DAG.

ğŸ¯ Key Learnings
API-based data ingestion

Airflow DAG orchestration

ETL pipeline design

Dockerized data pipelines

PostgreSQL integration

ğŸ”® Future Improvements
Historical weather ingestion

Incremental loading

Data quality checks

Cloud deployment

Analytics dashboards

ğŸ‘¤ Author
Robin
Aspiring Data Engineer
GitHub: https://github.com/Robin6551

yaml
Copy code

---

## 4ï¸âƒ£ Commit README
```powershell
git add README.md .gitignore
git commit -m "Add Weather pipeline README"
git push
